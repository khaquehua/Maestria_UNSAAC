---
title: "Análisis Multivariado"
author: "Kevin Heberth Haquehua Apaza"
date: "23 de julio del 2025"
output:
  word_document:
    reference_docx: template.docx
    toc: true
    toc_depth: 3
subtitle: 'Producto académico 02'
---

# Examen AED multivariado, ACP y AFE

## Ejercicio 1:

Sea $\mathbf{X}$ un vector aleatorio con matriz de varianza-covarianza dada por

$$
\Sigma = \begin{pmatrix}
4 & 1 \\
1 & 3
\end{pmatrix}
$$

a) Determine las componentes principales $Y1$ y $Y2$.
b) Calcule la proporción de la varianza total explicada por la primera componente principal.
c) Calcule la matriz de correlaciones a partir de la matriz de covarianzas y determine las componentes principales $Z1$ y $Z2$ a partir de \textbf{R}. Calcule la proporción de la varianza total explicada por $Y1$
d) Calcule la correlación entre las variables $X_i$ y las componentes principales, es decir, calcule

$$
{\rho}_{(X_{1}, Z_{1})}, {\rho}_{(X_{1}, Z_{2})}, {\rho}_{(X_{2}, Z_{1})}
$$


### Solución

a y b)

c) **Calcule la matriz de correlaciones a partir de la matriz de covarianzas y determine las componentes principales $Z1$ y $Z2$ a partir de \textbf{R}. Calcule la proporción de la varianza total explicada por $Y1$**

```{r}
S <- matrix(c(4, 1, 1, 3), 2, 2) ; S
```

Calcular la matriz de correlaciones

```{r}
D_inv <- diag(1 / sqrt(diag(S)))
R <- D_inv %*% S %*% D_inv
R
```

Ahora hallar los componentes principales

```{r}
pca <- princomp(covmat = R, cor = TRUE)
```

Y las varianzas explicadas

```{r}
explained_var <- pca$sdev^2 / sum(pca$sdev^2)
explained_var
```

En el cálculo manual no hya tanta diferencia, debido al cálculo con decimales.

d) **Calcule la correlación entre las variables $X_i$ y las componentes principales, es decir, calcule**

$$
{\rho}_{(X_{1}, Z_{1})}, {\rho}_{(X_{1}, Z_{2})}, {\rho}_{(X_{2}, Z_{1})}
$$

```{r}
L <- unclass(pca$loadings) ; L
```

```{r}
# Autovalores
lambda <- pca$sdev^2

# Correlaciones
cor <- L %*% diag(sqrt(lambda))
cor
```

## Ejercicio 2:

La siguiente tabla muestra los datos sobre la longitud de huesos registrados de 20 jóvenes a los 8, 8.5, 9 y 9.5 años respectivamente; Verificar si alguno de los individuos es considerado un dato atípico multivariado. Realizar la comprobación paso a paso como se realizó en clase (matricialmente), además tienes que comprobarlo con la función directa en el R

```{r}
data <- data.frame(X1_y8 = c(47.8, 46.4, 46.3, 45.1, 47.6, 52.5, 51.2, 49.8,
                             48.1, 45.0, 51.2, 48.5, 52.1, 48.2, 49.6, 50.7,
                             47.2, 53.3, 46.2, 46.3),
                   X2_y8.5 = c(48.8, 47.3, 46.8, 45.3, 48.5, 53.2, 53.0, 50.0,
                               50.8, 47.0, 51.4, 49.2, 52.8, 48.9, 50.4, 51.7,
                               47.7, 54.6, 47.5, 47.6),
                   X3_y9 = c(49.0, 47.7, 47.8, 46.1, 48.9, 53.3, 54.3, 50.3,
                             52.3, 47.3, 51.6, 53.0, 53.7, 49.3, 51.2, 52.7,
                             48.4, 55.1, 48.1, 51.3),
                   X3_y9.5 = c(49.7, 48.4, 48.5, 47.2, 49.3, 53.7, 54.5, 52.7,
                               54.4, 48.3, 51.9, 55.5, 55.0, 49.8, 51.8, 53.3,
                               49.5, 55.3, 48.4, 51.8))
```

### Solución

Hallemos manualmente

```{r}
# Calcular vector de medias
mu <- colMeans(data) ; mu

data_centered <- t(apply(data, 1, function(x) x - mu))
data_centered <- as.matrix(data_centered) ; data_centered

# Calcular matriz de covarianza e inversa
S <- cov(data) ; S
S_inv <- solve(S) ; S_inv

# Calcular distancia de Mahalanobis manualmente (paso a paso)
dist_manual <- (data_centered) %*% S_inv %*% t(data_centered)
dist_manual<-(diag(dist_manual)) ; dist_manual

```

Ahora hallemos con la función directa del R

```{r}
# Calcular con la función base de R
dist_r <- mahalanobis(data, center = mu, cov = S) ; dist_r
```

Comparemos

```{r}
# Comparar resultados
resultado <- data.frame(
  Observacion = rownames(data),
  Dist_Mahal_Manual = dist_manual,
  Dist_Mahal_R       = dist_r,
  Diferencia = abs(dist_manual - dist_r)
)

print(resultado)
```

Practimente no se observan diferencias significativas. Ahora veamos los individuos considerados atípicos

```{r}
# Umbral usando el percentil 97.5 de chi-cuadrado con 4 grados de libertad
umbral <- qchisq(0.975, df = 4) ; umbral

# Agregar al dataframe
resultado$dist_maha <- dist_r
resultado$Atipico <- ifelse(resultado$dist_maha > umbral, "Sí", "No")

# Mostrar resultado
print(resultado)
```

Se observa que no se tiene ningun dato atípicos registrado, sin embargo la observación 9, 11 y 20 están cercas a considerarse outliers multivariados.

## Ejercicio 3:

En el Excel _(pregunta3.xlsx)_ se muestran los valores de cinco variables obtenidas en 20 alumnos que quieren entrar a alguna universidad del consejo de rectores. Las variables en estudio son la distancia en kilómetros al lugar del colegio en el que estudiaban (DIST), el promedio de horas que hacían actividad física a la semana (EF), índice de masa corporal (IMC), IQ (coeficiente intelectual) y NEM (promedio de notas con el cual postulan a las universidades). Se quiere determinar las relaciones existentes entre dichas variables intentando reducir la dimensionalidad del problema vía un análisis factorial exploratorio.

a) Sobre el conjunto de datos halle si se cumple o no la normalidad multivariada.
b) Verifique si existe presencia de valores outliers para la data.
c) Realice el análisis factorial exploratorio vía componentes principales e interprete y justifique sus conclusiones.


### Solución

Leamos la data

```{r, warning=FALSE, echo=FALSE, include=FALSE}
library(here)
```


```{r}
library(readxl)
pregunta3 <- read_excel(here("pregunta3.xlsx"))
```

a) **Sobre el conjunto de datos halle si se cumple o no la normalidad multivariada.**

Veamos si los datos siguen una distribución multivariada con los test de Mardia, Henze-Zirkler y Royston

```{r}
#Libreria a utilizar
library(MVN)
```


```{r}
Y <- pregunta3[,-1]
```


```{r}
Mardia = mvn(Y, mvn_test = "mardia")
Mardia$multivariate_normality
```

La prueba de Mardia indica que no se tiene una distribución normal, sigamos con el test de Henze-Zirkler

```{r}
HZ =mvn(Y, mvn_test = "hz") 
HZ$multivariate_normality
```

El test de Henz indica que es normal, ahora el test de Royston

```{r}
Roy =mvn(Y, mvn_test = "royston") 
Roy$multivariate_normality
```

En dos se indican que no se tiene una distribución normal y en uno al momento se indico que sigue una distribución normal. Veamos con los otros test

```{r}
HW =mvn(Y, mvn_test = "hw") 
HW$multivariate_normality
```

```{r}
HW =mvn(Y, mvn_test = "doornik_hansen") 
HW$multivariate_normality
```

```{r}
HW =mvn(Y, mvn_test = "energy") 
HW$multivariate_normality
```

La mayor parte de los test indicaron que no se sigue una distribución normal multivariada, por lo tanto los datos no siguen una distribución normal multivariada. No se cumple la normalidad multivariada.

b) **Verifique si existe presencia de valores outliers para la data.**

Veamos primeramente a nivel univariado

```{r}
boxplot(pregunta3[,-1])
```

Parece que distancia y EF tienen valores outliers a nivel univariado.

Ahora veamos a nivel multivariado a través de la distancia clásica de Mahalanobich

```{r}
# Librerias a utilizar
library(mvoutlier)
library(aplpack)
```

```{r}
distances <- dd.plot(pregunta3[,-1], quan=1/2, alpha=0.025)
out1=data.frame(pregunta3,pred=as.numeric(distances$outliers))
subset(out1,pred==1)
```

Se muestran los individuos los cuales son outliers multivariados, ahora con la distancia de Mahalanobich robusta

```{r}
outliers <- aq.plot(pregunta3[,-1], delta=qchisq(0.975, df = ncol(pregunta3[,-1])), quan = 1/2, alpha = 0.05)
out2=data.frame(pregunta3,pred=as.numeric(outliers$outliers))
subset(out2,pred==1)
```

Se tienen los mismo individuos (2, 7, 12, 18, 19 y 20) como outliers multivariados.

c) **Realice el análisis factorial exploratorio vía componentes principales e interprete y justifique sus conclusiones.**

Ver si existen datos NA

```{r}
colSums(is.na(pregunta3))
```

No se tienen datos perdidos, empecemos realizando la prueba de esfericidad de Bartlet

```{r}
library(psych)
cortest.bartlett(cor(Y), n=nrow(Y))
```

El valor es significativo, justifica el uso de reducción de datos, veamos el test KMO

```{r}
KMO(Y)
```

Los valores son menores a 0.5 a excepción de Distancia [km] por lo que se puede extraer para que se considere aceptable la aplicación del análisis factorial al conjunto de datos

```{r}
data_AFE <- Y[,-1] 
```

Veamos el test de bartlet y KMO

```{r}
cortest.bartlett(cor(data_AFE), n=nrow(data_AFE))
```

Significativo, ahora veamos el KMO

```{r}
KMO(data_AFE)
```

Ahora si es justificable el uso de un análisis factorial exploratorio, realizemos con la rotación principal

```{r}
facto=principal(r=data_AFE,nfactors=4,rotate="none")
facto=principal(r=data_AFE,nfactors=4,rotate="varimax")
facto
facto$loadings

```

Por la mayor explicación de la varianza, se recomienda usar 3 factores

```{r}
facto=principal(r=data_AFE,nfactors=3,rotate="varimax")
facto
facto$loadings
```

Se tienen los siguientes factores

RC1 = IQ y NEM (Academico)
RC2 = IMC (Caracteristicas del Individuo)
RC3 = EF (Nivel de actividad fisica)

```{r}
biplot(prcomp(data_AFE, scale = FALSE))
abline(h = 0, v = 0, lty = 2, col = 8)
```


Saquemos ahora los scores

```{r}
scores <- as.matrix(data_AFE) %*% as.matrix(facto$loadings)
scores <- data.frame(scores) ; scores
```

Realizemos una transformación manteniendo sus características

```{r}
Zscores<-scale(scores)
transScore <- Zscores*100+500 # Proceso de baremación de PISA
transScore <- data.frame(transScore)
hist(transScore$RC1)
```

Recodifiquemos para la interpretación

#### RC1 (Academico)

```{r}
transScore$RNC1[transScore$RC1<350] <-1
transScore$RNC1[transScore$RC1>=350 & transScore$RC1<450] <-2
transScore$RNC1[transScore$RC1>=450 & transScore$RC1<550] <-3
transScore$RNC1[transScore$RC1>=550 & transScore$RC1<650] <-4
transScore$RNC1[transScore$RC1>=650] <-5


# Etiquetar
transScore$RNC1 <- factor(transScore$RNC1, 
                             labels = c("Pesima", "Mala", "Regular",
                                        "Buena", "Excelente"))


fi=table(transScore$RNC1)
probabilidad=prop.table(table(transScore$RNC1))*100
cbind(fi,probabilidad)
barplot(prop.table(table(transScore$RNC1)), col = "darkBlue", xlab = "Académico")

```

Se observa que la mayor parte se encuentra en un nivel malo y regular con respecto a la parte académica

#### RC2 (Características del individuo)

```{r}
transScore$RNC2[transScore$RC2<350] <-1
transScore$RNC2[transScore$RC2>=350 & transScore$RC2<450] <-2
transScore$RNC2[transScore$RC2>=450 & transScore$RC2<550] <-3
transScore$RNC2[transScore$RC2>=550 & transScore$RC2<650] <-4
transScore$RNC2[transScore$RC2>=650] <-5


# Etiquetar
transScore$RNC2 <- factor(transScore$RNC2, 
                             labels = c("Pesima", "Mala", "Regular",
                                        "Buena", "Excelente"))


fi=table(transScore$RNC2)
probabilidad=prop.table(table(transScore$RNC2))*100
cbind(fi,probabilidad)
barplot(prop.table(table(transScore$RNC2)), col = "darkBlue", xlab = "Características")

```

Se observa que la mayor parte se encuentra en un nivel malo y regular con respecto a las características de los individuos

#### RC3 (Nivel de actividad fisica)

```{r}
transScore$RNC3[transScore$RC3<350] <-1
transScore$RNC3[transScore$RC3>=350 & transScore$RC3<450] <-2
transScore$RNC3[transScore$RC3>=450 & transScore$RC3<550] <-3
transScore$RNC3[transScore$RC3>=550 & transScore$RC3<650] <-4
transScore$RNC3[transScore$RC3>=650] <-5


# Etiquetar
transScore$RNC3 <- factor(transScore$RNC3, 
                             labels = c("Pesima", "Mala", "Regular",
                                        "Buena", "Excelente"))


fi=table(transScore$RNC3)
probabilidad=prop.table(table(transScore$RNC3))*100
cbind(fi,probabilidad)
barplot(prop.table(table(transScore$RNC3)), col = "darkBlue", xlab = "Factor1")

```

Se observa que la mayor parte se encuentra en un nivel regular, bueno y malo, más balanceado con respecto a la parte del nivel de actividad física
