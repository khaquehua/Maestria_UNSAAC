---
title: "Análisis Multivariado"
author: "Kevin Heberth Haquehua Apaza"
date: "30 de julio del 2025"
output:
  word_document:
    reference_docx: template.docx
    toc: true
    toc_depth: 3
subtitle: 'Producto académico 03'
---

# Ejercicios AFE, Cluster y análisis de correspondencia múltiple

## CASO 1: Postulantes (6 puntos)

Se tienen datos de las notas de alumnos postulantes a un colegio de alto rendimiento, se desea agrupar las notas de los cursos y ver que grupos podrían haber de cursos.

Las notas de los siguientes cursos son Razonamiento verbal, Razonamiento matemático, Matemáticas, Psicología y filosofía, Física, Lógica, Biología, Historia y Química.

Archivo a utilizar **postulantes.sav**

```{r, warning=FALSE, echo=FALSE, include=FALSE}
library(here)
```

```{r}
library(foreign)
postulantes <- read.spss(here("9 Analisis Multivariado/Trabajo 3/Postulantes.sav"),
                 to.data.frame = TRUE,
                 use.value.labels = TRUE)
```

a) Realizar análisis factorial exploratorio
b) Decida cuantos factores retener explique el por qué.
c) Decida el método de rotación y explique el por qué.
d) Explicar los resultados y de sus conclusiones del ejercicio.

### Solución

a) **Realizar análisis factorial exploratorio**

Veamos un resumen de los datos

```{r}
summary(postulantes)
```

No se tienen datos vacíos por lo cual no es necesario realizar una técnica de imputación, por lo que empezemos omitiendo la primera columna que representa a los códigos de los postulantes

```{r}
postulantes <- postulantes[,-1]
```

#### Prueba de esfericidad de Bartlet

```{r}
library(psych)
cortest.bartlett(cor(postulantes), n=nrow(postulantes))
```

Se nos muestra un p valor menor a 0.05, justifica el uso de reducción de datos.

#### Indicador Kaiser-Meyer-Olkinn KMO y MSA

```{r}
KMO(postulantes)
```

Los valores son menores a 0.5 a excepción de PSI (Psicología) por lo que se puede extraer para que se considere aceptable la aplicación del análisis factorial al conjunto de datos

```{r}
data_AFE <- postulantes[,-4] 
```

Veamos nuevamente el test de bartlet y KMO

```{r}
cortest.bartlett(cor(data_AFE), n=nrow(data_AFE))
```

Significativo, ahora veamos el KMO

```{r}
KMO(data_AFE)
```

Ahora si es justificable el uso de un análisis factorial exploratorio

b) **Decida cuantos factores retener explique el por qué.**

Empezemos realizando la primera seleccionando tomando en cuenta todos los factores

```{r}
facto=principal(r=data_AFE,nfactors=8,rotate="none")
facto
facto$loadings
```

Por la mayor explicación de la varianza, se recomienda usar 3 o 4 factores. Decidamos el uso de 4 factores

c) **Decida el método de rotación y explique el por qué.**

Teniendo en cuenta que se tendrán 4 factores veamos las cargas factoriales

```{r}
facto=principal(r=data_AFE,nfactors=4,rotate="none")
facto
facto$loadings
```

Vemos que las cargas aportan a cada factor siendo posible su distinción entre factores teniendo los siguientes resultados

- PC1: **RM (confuso)**, MAT, FIS, BIO **HIS (confuso)** y QUI
- PC2: RV, **HIS (confuso)**
- PC3: LOG
- PC4: **RM (confuso)** y BIO

Siendo casos en donde no se observan las diferencias de una variable hacia el factor, por lo que es necesario explicar la máxima varianza, veamos con varimax

```{r}
facto=principal(r=data_AFE,nfactors=4,rotate="varimax")
facto
facto$loadings
```

Veamos ahora los factores, asimismo como su importancia

- RC1: FIS, BIO y QUI
- RC2: RV e HIS
- RC4: RM, MAT
- RC3: LOG

```{r}
biplot(prcomp(data_AFE, scale = FALSE))
abline(h = 0, v = 0, lty = 2, col = 8)
```

Como se observa hay mejor distinción de las variables con respecto a sus factores asimismo podemos explicar cada factor de la siguiente manera

- **RC1 (Ciencias naturales):** Conformada por Física, biología y química
- **RC2 (Comprensión información):** Conformada por razonamiento verbal e historia
- **RC4 (Ciencias matemáticas):** Conformada por razonamiento matemático y matemáticas
- **RC3 (Lógica):** Conformada por lógica

Concluyendo que la rotación varimax permite una mejor distinción entre factores a través de la explicación de su varianza máxima asimismo como los componentes creados tienen lógica con el contexto del ejercicio.

d) **Explicar los resultados y de sus conclusiones del ejercicio.**

Saquemos ahora los scores

```{r}
scores <- as.matrix(data_AFE) %*% as.matrix(facto$loadings)
scores <- data.frame(scores) ; head(scores)
```

Realizemos una transformación manteniendo sus características

```{r}
Zscores<-scale(scores)
transScore <- Zscores*100+500 # Proceso de baremación de PISA
transScore <- data.frame(transScore)
```

Recodifiquemos para la interpretación

#### RC1 (Ciencias naturales)

```{r}
transScore$RNC1[transScore$RC1<350] <-1
transScore$RNC1[transScore$RC1>=350 & transScore$RC1<450] <-2
transScore$RNC1[transScore$RC1>=450 & transScore$RC1<550] <-3
transScore$RNC1[transScore$RC1>=550 & transScore$RC1<650] <-4
transScore$RNC1[transScore$RC1>=650] <-5


# Etiquetar
transScore$RNC1 <- factor(transScore$RNC1, 
                             labels = c("Pesima", "Mala", "Regular",
                                        "Buena", "Excelente"))


fi=table(transScore$RNC1)
probabilidad=prop.table(table(transScore$RNC1))*100
cbind(fi,probabilidad)
barplot(prop.table(table(transScore$RNC1)), col = "darkBlue", xlab = "Ciencias naturales")

```

Se observa que la mayor parte se encuentra en un nivel regular y bueno en la parte de ciencias naturales, seguido de malos y pésimos y por último una pequeña parte tiene notas excelentes en las ciencias naturales.

#### RC2 (Comprensión de la información)

```{r}
transScore$RNC2[transScore$RC2<350] <-1
transScore$RNC2[transScore$RC2>=350 & transScore$RC2<450] <-2
transScore$RNC2[transScore$RC2>=450 & transScore$RC2<550] <-3
transScore$RNC2[transScore$RC2>=550 & transScore$RC2<650] <-4
transScore$RNC2[transScore$RC2>=650] <-5


# Etiquetar
transScore$RNC2 <- factor(transScore$RNC2, 
                             labels = c("Pesima", "Mala", "Regular",
                                        "Buena", "Excelente"))


fi=table(transScore$RNC2)
probabilidad=prop.table(table(transScore$RNC2))*100
cbind(fi,probabilidad)
barplot(prop.table(table(transScore$RNC2)), col = "darkBlue", xlab = "Comprension de la informacion")

```

Se observa que respecto a comprension de la información la mayor parte se encuentra en un nivel regular, seguido de bueno, mala, pésima y una pequeña parte en el nivel excelente.

#### RC4 (Ciencias matemáticas)

```{r}
transScore$RNC4[transScore$RC4<350] <-1
transScore$RNC4[transScore$RC4>=350 & transScore$RC4<450] <-2
transScore$RNC4[transScore$RC4>=450 & transScore$RC4<550] <-3
transScore$RNC4[transScore$RC4>=550 & transScore$RC4<650] <-4
transScore$RNC4[transScore$RC4>=650] <-5


# Etiquetar
transScore$RNC4 <- factor(transScore$RNC4, 
                             labels = c("Pesima", "Mala", "Regular",
                                        "Buena", "Excelente"))


fi=table(transScore$RNC4)
probabilidad=prop.table(table(transScore$RNC4))*100
cbind(fi,probabilidad)
barplot(prop.table(table(transScore$RNC4)), col = "darkBlue", xlab = "Ciencias matemáticas")

```

Se observa que la mayor parte se encuentra en un nivel regular y bueno, seguido de malo y pésimo, y una pequeña parte tiene un término excelente con respecto a los resultados enfocados a las ciencias matemáticas.

#### RC3 (Lógica)

```{r}
transScore$RNC3[transScore$RC3<350] <-1
transScore$RNC3[transScore$RC3>=350 & transScore$RC3<450] <-2
transScore$RNC3[transScore$RC3>=450 & transScore$RC3<550] <-3
transScore$RNC3[transScore$RC3>=550 & transScore$RC3<650] <-4
transScore$RNC3[transScore$RC3>=650] <-5


# Etiquetar
transScore$RNC3 <- factor(transScore$RNC3, 
                             labels = c("Pesima", "Mala", "Regular",
                                        "Buena", "Excelente"))


fi=table(transScore$RNC3)
probabilidad=prop.table(table(transScore$RNC3))*100
cbind(fi,probabilidad)
barplot(prop.table(table(transScore$RNC3)), col = "darkBlue", xlab = "Lógica")

```

Se observa que respecto a lógica la mayor parte se encuentra en un nivel regular, seguido de bueno, mala, pésima y una pequeña parte en el nivel excelente.

## CASO 2: Agrupando clientes mayoristas (7 puntos):

El conjunto de datos se refiere a los clientes de un distribuidor mayorista de Portugal, el cual comercializa distintos tipos de productos.

Cada una de las observaciones hace referencia a un cliente distinto, el cual incluye el gasto anual en unidades monetarias (u.m.) para cada una de las categorías.

Se nos solicita realizar un análisis clúster que nos permita agrupar a nuestros clientes en función de los distintos tipos de productos que adquirieron, para lo cual contamos:

| Variable            | Descripción                                                                |
|---------------------|----------------------------------------------------------------------------|
| `Channel`           | Canal de clientes: 1. Horeca (Hotel/Restaurante/Café) 2. Canal Minorista   |
| `Región`            | Región de los clientes: 1. Lisboa, 2. Oporto y 3. Otra                     |
| `Fresh`             | Gasto anual en productos frescos.                                          |
| `Milk`              | Gasto anual en productos lácteos.                                          |
| `Grocery`           | Gasto anual en productos comestibles.                                      |
| `Frozen`            | Gasto anual en productos congelados.                                       |
| `Detergent_Papers`  | Gasto anual en detergentes y productos de papel.                           |
| `Delicatessen`      | Gasto anual en productos preparados (snacks y licor).                      |

Los datos se encuentran en el archivo "clientes.csv".

```{r}
clientes <- read.csv(here("9 Analisis Multivariado/Trabajo 3/clientes.csv"))
```


Luego de cargar el conjunto de datos en R, realizar las 2 opciones que se presenta:

**Opción 1**:

1. Generar un nuevo dataset solo con las variables numéricas y estandarizarlas.
2. Generar el agrupamiento por particiones utilizando el método kmeans con k=4.
3. Añadir el dataset original la columna cluster, que identificará a los grupos que obtuvimos mediante esta metodología.
4. Graficar y perfilar a nuestros clientes según su agrupación.

**Opción 2**:

1. Generar un nuevo dataset solo con las variables numéricas y estandarizarlas.
2. Encuentre ahora los clusters de forma jerárquica, calculando la matriz de distancias euclidianas y seleccionando en enlace que creas mejor se ajuste a los datos.
3. Comparar los métodos de enlace y determinar cuál es el adecuado.
4. Generar el nuevo agrupamiento jerárquico con el enlace seleccionado.
5. Graficar el dendograma respectivo y determinar el número de clusters.
6. Graficar y perfilar a nuestros clientes según su agrupación jerárquica.

### Solución

**Opción 1**:

1. **Generar un nuevo dataset solo con las variables numéricas y estandarizarlas.**
2. **Generar el agrupamiento por particiones utilizando el método kmeans con k=4.**
3. **Añadir el dataset original la columna cluster, que identificará a los grupos que obtuvimos mediante esta metodología.**
4. **Graficar y perfilar a nuestros clientes según su agrupación.**

**Opción 2**:

1. **Generar un nuevo dataset solo con las variables numéricas y estandarizarlas.**
2. **Encuentre ahora los clusters de forma jerárquica, calculando la matriz de distancias euclidianas y seleccionando en enlace que creas mejor se ajuste a los datos.**
3. **Comparar los métodos de enlace y determinar cuál es el adecuado.**
4. **Generar el nuevo agrupamiento jerárquico con el enlace seleccionado.**
5. **Graficar el dendograma respectivo y determinar el número de clusters.**
6. **Graficar y perfilar a nuestros clientes según su agrupación jerárquica.**

## CASO 3: (7 puntos)

Investigar y realizar un informe monográfico sobre el **_análisis de correspondencia múltiple_** adjuntar un ejercicio aplicando R o Phyton.

### Solución

Aca va la solución
