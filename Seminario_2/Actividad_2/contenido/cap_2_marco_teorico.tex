\newpage
\chapter{MARCO TEÓRICO CONCEPTUAL}

\section{Bases Teóricas}

\subsubsection{Axiomas de probabilidad y notaciones básicas}

\begin{definition}
    \textbf{Notaciones} \citep{koski2011bayesian}
    \begin{itemize}
        \item El conjunto de todos los posibles resultados de un experimento aleatorio es denotado por $\Omega$
        \item El resultado de un posible experimento aleatorio o un conjunto del resultado es representado por $X$
        \item El espacio de parámetros es denotado por $\tilde{\Theta}$, tal que $\Omega = X \times \tilde{\Theta}$
        \item Si $A$ y $B$ son dos conjuntos, luego $A \cup B$ representa la unión, además que si ${A}_{1},{A}_{2},\cdots,{A}_{n}$ son una colección finita de conjuntos, entonces $\bigcup_{i=1}^{n} {A}_{i}$ representa la unión de conjuntos.
        \item Si $A$ y $B$ son dos conjuntos, luego $A \cap B$ representa la intersección, además que si ${A}_{1},{A}_{2},\cdots,{A}_{n}$ son una colección finita de conjuntos, entonces $\bigcap_{i=1}^{n} {A}_{i}$ representa la intersección de conjuntos.
        \item $A \subset B$ denota que $A$ es un subconjunto de $B$. $A \subseteq B$ denota que $A$ es un subconjunto de $B$, posiblemente igual a $B$
        \item El conjunto vacío puede ser denotado por $\emptyset$
        \item ${A}^{c}$ denota el complemento de $A$ 
    \end{itemize}
\end{definition}

\begin{definition}
    \textbf{Probabilidad} \citep{koski2011bayesian}
    La distribución de probabilidad $P$ es una función que satisface los axiomas de Kolmogorov
    \begin{enumerate}
        \item $P(\emptyset) = 0$ y $P(\Omega) = 1$
        \item Si ${A}_{1},{A}_{2},\cdots,{A}_{n}$ es una colección finita tal que para cada $A_i$ como eventos que satisface ${A}_{j} \cap {A}_{k} = \emptyset$ para todo $j \neq k$, entonces
        $$P(\bigcup_{i=1}^{n} {A}_{i})=\sum\limits_{i = 1}^{n}P(A_i)$$ 
        \item $0 \leq P(A) \leq 1$ para todo $A$ que pertenece a $\Omega$
    \end{enumerate}
\end{definition}

\begin{definition}
    \textbf{Distribución de probabilidad sobre $X$} \citep{koski2011bayesian}
    Si $X$ contiene un número finito de elementos $x$, una distribución de probabilidad sobre $X$ satisface
    \begin{enumerate}
        \item $P(x) \geq 0$ para todo $x \in X$
        \item $\sum\limits_{x \in X}^{}P(x) = 1$
    \end{enumerate}
\end{definition}

\begin{definition}
    \textbf{Regla de Bayes} \citep{koski2011bayesian}
    La regla de Bayes para dos eventos $A$ y $C$ es dada por
    $$P(A/C) = \frac{P(C/A)P(A)}{P(C)}$$
\end{definition}

\subsubsection{Análisis de datos bayesiano}
La característica esencial de los métodos bayesianos es uso de la probabilidad para cuantificar la incertidumbre en inferencias basadas en el análisis de datos estadísticos. En los que \cite{gelman1995bayesian} los divide en estos tres pasos:
\begin{enumerate}
    \item Configurar un modelo de probabilidad completo en el cual se debe de tener una distribución de probabilidad conjunta para todas cantidades observadas y no observadas en un problema, en el cual el modelo debe ser coherente con el conocimiento científico subyacente y el proceso de recopilación de datos.
    \item El condicionamiento sobre datos observados en el que se debe calcular e interpretar la distribución aposteriori apropiada en el que la distribución de probabilidad no observada como último interés que va a estar dado por los datos observados.
    \item Evaluación del ajuste del modelo y las implicaciones de las distribuciones aposteriori resultante, planteando las siguientes preguntas: ¿Qué tan bien se ajusta el modelo a los datos?¿Son razonables las conclusiones sustanciales?¿Qué tan sensibles son los resultados a los supuestos del modelo del primer paso?. En respuesta se puede modificar o expandir el modelo y repetir los tres pasos.
\end{enumerate}

\subsubsection{Inferencia Bayesiana}
Las conclusiones estadísticas bayesianas acerca de un parámetro desconocido $\theta$ de datos no observados $\tilde{y}$ son explicados en términos de probabilidad que son condicionadas de los valores observados de $y$, en la que su notación puede ser escrito como $P(\theta | x)$, también se condiciona los valores conocidos de cualquier covariable $x$ \cite{gelman1995bayesian}

\subsubsection{Modelo de verosimilitud}\citep{lawson2018bayesian}
La verosimilitud dado los datos $y_i$ donde $i = 1, \cdots, m$ es definido como
$$
L(\mathbf{y}|\theta) = \prod\limits_{i = 1}^{m}f({y}_{i}|\theta)
$$
Donde $\theta$ es un vector de longitud $p$: $\{ {\theta}_{1}, {\theta}_{2}, \cdots , {\theta}_{p} \}$ y $f(\cdot | \cdot)$ es una función de densidad (o masa), es posible calcular el producto de las contribuciones individuales ya que los valores de la muestra $\mathbf{y}$ dado los parámetros son independientes. El logaritmo de verosimilitud es importante en el desarrollo de modelos y es definido como:
$$
l(\mathbf{y}| \theta) = \sum\limits_{i = 1}^{m}\text{log }f({y}_{i}| \theta)
$$

\subsubsection{Distribución a priori}
Todos los parámetros de los modelos bayesianos son estocásticos y son asignados apropiadamente por las distribuciones de probabilidad, la distribución a priori se asigna al parámetro antes de ver los datos, además que las distribuciones a priori es que proporcionan datos adicionales a un problema y que pueden usarse para mejorar la estimación o la identificación de los parámetros \citep{lawson2018bayesian}.

Para un vector de parámetros $\theta$, la distribución a priori puede ser denotada como $\mathbf{g}(\theta)$

\subsubsection{Distribución a posteriori}
Las distribuciones a priori y la verosimilitud proporcionan dos fuentes de información acerca de cualquier problema. La verosimilitud informa sobre el parámetro con los datos, mientras que la distribución a priori informa sobre las creencias o informaciones previas \citep{lawson2018bayesian}. Cuando hay grandes cantidades de datos, es decir un tamaño de muestra grande, la probabilidad contribuirá más en la estimación del riesgo relativo, cuando se tienen pocos datos, la distribución a priori dominará el análisis.

El producto de la verosimilitud y la distribución a priori es llamado como la distribución a posteriori, en el cual esta distribución describe los parámetros dado los datos observados y poder realizar suposiciones previas, esta es definida como:
$$
P(\theta | \mathbf{y}) = L(\mathbf{y} | \theta)\mathbf{g(\theta)/C}
$$
Donde $C = \int_{P}^{}L(\mathbf{y}|\theta)\mathbf{g}(\theta)d\theta$ con $\mathbf{g}(\theta)$ como la distribución conjunta de los vectores $\theta$. Alternativamente este vector puede ser especificada de manera proporcional: $P(\theta | \mathbf{y}) \propto L (\mathbf{y} | \theta)\mathbf{g}(\theta)$

\subsubsection{Conjugación}
Ciertas combinaciones de distribuciones a priori y verosimilitudes conducen a una distribución a posteriori que la distribución a priori. Esto genera ventajas en la inferencia, ya que la forma a posteriori se desprenderá de la especificación previa \citep{lawson2018bayesian}. La Tabla \ref{tabla:distribuciones} muestra unos ejemplos de conjugación.

\begin{table}[htb]
\centering
\begin{tabular}{|p{4cm}|p{4cm}|p{7cm}|}
\hline
\textbf{Verosimilitud} & \textbf{Priori}& \textbf{Posteriori} \\ \hline
$\mathbf{y} \sim Poisson(\theta)$ & $\theta \sim G(\alpha, \beta)$ & $\theta|\mathbf{y} \sim G(\sum{y}_{i} + \alpha,m + \beta)$ \\
$\mathbf{y} \sim binomial(\mathbf{p},1)$ & $\mathbf{p} \sim Beta({\alpha}_{1}, {\alpha}_{2})$ & $\mathbf{p}|\mathbf{y} \sim Beta(\sum{y}_{i} + {\alpha}_{1},m - \sum {y}_{i} + {\alpha}_{2})$ \\
$\mathbf{y} \sim N(\mu, \tau), \tau \text{ fijo}$ & $\mu \sim N({\alpha}_{0}, {\tau}_{0})$ & $\mu|\mathbf{y} \sim N\left( \frac{{\tau}_{0}\sum {y}_{i} + {\alpha}_{0} \tau}{m {\tau}_{0} + \tau}, \frac{{\tau}_{0}\tau}{m{\tau}_{0}+\tau} \right)$ \\
$\mathbf{y} \sim gamma(1,\beta)$ & $\beta \sim G({\alpha}_{0}, {\beta}_{0})$ & $\beta|\mathbf{y} \sim G(1 + {\alpha}_{0}, {\beta}_{0} + \sum{y}_{i})$ \\
\hline
\end{tabular}
\caption{Ejemplos de distribuciones conjugadas}
\label{tabla:distribuciones}
\end{table}

Por lo que la elección de la distribución a priori de los parámetros es muy importante ya que puede afectar a la distribución a posteriori significativamente, asimismo el equilibro entre la evidencia a priori y posteriori es relacionado a la verosimilitud y el tamaño de muestra.

\subsubsection{Distribuciones predictivas}
Las distribuciones a posteriori resume la comprensión de los parámetros dado los datos observados y toma un rol fundamental en el modelado bayesiano. Sin embargo también se puede examinar otras distribuciones relacionadas que pueden ser útiles cuando se requieren la predicción de nuevos datos \citep{lawson2018bayesian}, si definimos una nueva observación de $y$ como $y^*$. Podemos determinar la distribución predictiva de $y^*$ de dos formas. En general la distribución predictiva es definida como
$$
P(y^* | \mathbf{y}) = \int_{}^{} L(y^* | \theta)P(\theta | \mathbf{y})d\theta
$$
Aqui la predicción es basada en la marginal sobre los parámetros en la verosimilitud de los nuevos datos $L(y^* | \theta)$ usando la distribución a posteriori $P(\theta | y)$ para definir la contribución de los datos observados en la predicción. Definida como distribución predictiva posteriori. Una variante de esta definición usan la distribución a priori en lugar de la distribución a posteriori.
$$
P(y^* | \mathbf{y}) = \int_{}^{} L(y^* | \theta)P(\theta)d\theta
$$
Esto enfatiza la predicción basada solo en la distribución a priori (antes de ver algún dato), la cual simplemente es la distribución marginal de $y^*$

\subsubsection{Modelos Jerárquicos Bayesianos}
En el modelado bayesiano, los parámetros libres tienen distribuciones. Estas distribuciones controlan la forma de los parámetros y son especificadas por el investigador basado generalmente en creencias previas sobre su comportamiento \citep{lawson2018bayesian}. 

La idea de que los valores de los parámetros podrían surgir de distribuciones es una característica fundamental de la metodología bayesiana y conduce al uso de modelos donde los parámetros surgen dentro de jerarquías

\subsubsection{Redes bayesianas}
Las redes bayesianas constituyen el refinamiento de un método de diagnóstico probabilístico conocido como el método probabilístico clásico \citep{koski2011bayesian}, tomemos en cuenta para el diagnóstico de enfermedades los siguientes conceptos:

Si se desea diagnosticar $n$ enfermedades ${E}_{1},{E}_{2},{E}_{3}, \cdots, {E}_{n}$ a partir de un conjunto de $m$ hallazgos ${H}_{1}, {H}_{2}, {H}_{3}, \cdots, {H}_{m}$ mostrado en el grafo se tiene una representación en la Figura \ref{figura:red_bayesiana}

\begin{figure}[htb]
\centering
\includegraphics[scale=0.7]{images/redes_bayesianas.png}
\caption{Grafo de una red bayesiana entre enfermedades y hallazgos}
\label{figura:red_bayesiana}
\end{figure}

Entonces el objetivo sería calcular las probabilidades del tipo:

$$
P({E}_{1}, {E}_{2}, \cdots, {E}_{n} | {H}_{1}, {H}_{2}, \cdots, {H}_{m})
$$

Estas probabilidades refieren a las probabilidades de cada enfermedad si están presentes o ausentes cuando se conoce una serie de observaciones asociadas a cada uno de los distintos hallazgos, entonces es necesario conocer:

\begin{itemize}
\item Probabilidad a priori de las distintas enfermedades
\item Probabilidad de que se obtengan determinadas observaciones asociadas a los distintos hallazgos conocida como la presencia o ausencia de las distintas enfermedades, es decir la verosimilitud.
\end{itemize}
En la cual integrando todas las probabilidades por la regla de Bayes se tiene:
$$
P({E}_{1},{E}_{2},\cdots,{E}_{n}|{H}_{1},{H}_{2},\cdots,{H}_{m}) = \frac{P({H}_{1},{H}_{2},\cdots,{H}_{m}|{E}_{1},{E}_{2},\cdots,{E}_{n})P({E}_{1},{E}_{2},\cdots,{E}_{n})}{\sum\limits_{{E}_{1}^{c},\cdots,{E}_{n}^{c} }^{}P({H}_{1},{H}_{2},\cdots,{H}_{m}|{E}_{1}^{c},{E}_{2}^{c},\cdots,{E}_{n}^{c})P({E}_{1}^{c},{E}_{2}^{c},\cdots,{E}_{n}^{c})}
$$

\section{Marco conceptual}
\begin{itemize}
\item \textbf{Queratocono:} Transtorno corneal ectásico, caracterizado por adelgazamiento progresivo del estroma y debilitamiento estructural con protrusión apical en forma de cono, llevando a defectos refractivos como miopía progresiva, astigmatismo irregular, cicatrización y disminución de la visión \citep{caruso2024corneal}

\item \textbf{Modelos bayesianos:} Modelos basados en la regla de Bayes en las cuales se estima la probabilidad a posteriori basada en información previa y verosimilitud de los datos \citep{koski2011bayesian}

\item \textbf{Redes bayesianas:} Representación gráfica de dependencias para razonamiento probabilístico, en la cual los nodos son las variables aleatorias y representa dependencia directa entre las variables \citep{sucar2006redes}
\end{itemize}

\section{Antecedentes}

\subsubsection{Antecedentes Internacionales}
El estudio de \cite{waddell2023applying} aplicó las redes bayesianas para el apoyo en la toma de decisiones, tomando datos de diferentes fuentes para explorar fenotípicos específicos y resultados clínicos, en la cual se pueden tener deficiencias debido al conjunto de datos pequeños y cohortes desequilibrados llegando a realizar inferencias falsas probabilísticas. Algo importante de destacar es como el uso de redes bayesianas ayuda en la detección de enfermedades especialmente en incertidumbre compleja, asimismo el autor indicar de que a pesar que la medicina actualmente se encuentra en competencia con el desarrollo de la inteligencia artificial, esta a su vez debe ser poder interpretada por los médicos en el cual el verdadero potencial de las redes bayesianas viene a ser la capacidad de hablar el lenguaje clínico, modelar preguntas relacionales causales y responder preguntas al explorar fenotipos individuales llegando a plantear preguntas cruciales como ¿por qué? y ¿que pasaria sí?. \\

El trabajo de \cite{ferez2017redes} presenta una mejor evidencia de la medicina basada en la evidencia en el cual se debe tener un uso consciente, explícito y juicioso de la evidencia científica disponible para tomar la decisión de los pacientes, en el cual comprender la realidad y expresarla de manera sistemática, inteligible y sintética. Integrar la experiencia de los profesionales con la evidencia científica disponible para mejorar la toma de decisiones clínicas. Para conseguir este propósito se tienen diferentes entes matemáticas en la que su estudio enfatizó a las redes bayesianas como una de las herramientas más valiosas en el proceso de toma de decisiones. El trabajo planteo conceptos probabilísticos enfocado al análisis bayesiano en los cuales se fundamentan las redes bayesianas, para posteriormente crear dos redes a ejemplos prácticos, en la cual la red bayesiana tiene una capacidad para representar la relación causa entre variables involucradas en un problema y visualizarlas de una manera muy intuitiva, el cual es el modelo gráfico probabilístico más utilizado en el marco de la toma de decisión diagnóstica.

\subsubsection{Antecedentes Nacionales}
La investigación de \cite{lopezmodelamiento} para optar el título de maestro en Estadística de la Pontífice Universidad Católica del Perú, se basó acerca de las infecciones respiratorias, en la cual buscó establecer una relación entre la incidencia de infecciones respiratorias agudas (IRA) y la incidencia de neumonía en el Perú en la cual además de ver si estas variables están correlacionadas quiso dar un enfoque espacial a nivel provincial en la cual se esperaría que la incidencia de (IRA) y neumonía sea mayor en provincias vecinas. Por lo cual se estudió la distribución espacial entre la incidencia de (IRA) y neumonía a nivel provincial en el Perú a través de un modelo espacial multivariado con efectos aleatorios condicionales autoregresivos utilizando inferencia bayesiana en el modelo jerárquico espacial multivariado a través del método de integración anidada de Laplace (INLA). El cual mostró una bondad de ajuste del modelo y su eficacia en la simulación realizada teniendo una optimización de tiempo menor, asimismo se vio una correlación positiva directa entre las personas con (IRA) sin neumonía y neumonía. Además de que se tuvo evidencia estadística entre las precipitaciones y la neumonía, indicando que el aumento de precipitaciones en una provincia reduce significativamente la incidencia de IRA sin neumonía, mientras el aumento de precipitación en una provincia aumenta la incidencia de neumonías en dicha provincia, teniendo una autocorrelación espacial moderada con respecto a las tasas de enfermedad entre provincias vecinas, recomendado que se puede trabajar con más covariables y factores que expliquen mejor la causalidad del modelo, de la misma forma incluir un modelo espacio-temporal empleando INLA que tiene una mejor eficiencia computacional al trabajar con más datos.

























